[ Audio is muted. Audio is muted. Audio is muted. >> Okay. So this week we're moving away from object oriented UML modeling. Now we're going -- now we're getting on to testing. So software testing. This lecture is mainly about verification and validation with some other bits and bobs to make sure that you -- that verification and validation that your software is verified and validated. That is basically ensuring that a software system meets a user's needs. So the objectives are to introduce software verification and validation, to describe the program inspection process, to explain static analysis as a verification technique, and I'm not really sure about that last one. So verification and validation sound very similar, but they are actually two different things. Verification is are we building the product right? So does your product actually conform to the specification? And validation is are we building the right product basically? Are we building the thing that the user actually needs? So when you do your testing, verification should check that the program meets its specification as you -- as we've learned how to like identify in the previous lab -- at the lab's lectures. And that it should meet its functional and nonfunctional requirements. Whereas validation, as I just said, makes sure that we meet the customer's expectations. So as we've seen in the previous lectures, that your specification doesn't always accurately reflect the needs of users. And you just -- so we go further than just making sure that have I built the correct software according to the specification? And also have I made -- have I gone further and made sure that the user actually gets what they want -- or need rather, because the user won't always tell you what they need. So during the whole software process, you should be checking that and verification and -- well, you should be applying verification and validation throughout the whole software process. So the point of this is so you can identify these two things. So discovering defects in the system and whether or not the system is actually usable. So we have two types of verification, and that is static verification and also dynamic verification. So the way we do static verification is actually basically just looking at the code and making sure that -- as if you're just reading it and identifying where your code is bad, where your specification and design is bad, how can it be improved, and that doesn't actually involve running the code. When you do -- when you are running the code, that will be software testing, and that's where you're actually concerned with what your product is doing. It's behavior, rather. So the first one, you know, you mostly do with looking at documents and looking at the code yourself. And the second one is where you get test data and making sure that you meet all the operational needs. So here's an example of a bad code. Does anyone -- would anyone like to give me an idea of what -- just one line of why this code is bad? It's okay. If not, just have a think anyway. Yeah? I mean, yeah, for code readability, yeah. Yeah, yeah, yeah, yeah. I'll allow that. I think that is actually one of the answers. There's other issues in there as well. So -- but for code readability, absolutely, yeah. Variable names, yes. Definitely. Why pass this as a parameter here, init temp, and then just call it X. You could call that something better. Yeah? What's wrong with the comment? [ Inaudible ] You're right. It's not very clear what the comment is saying about the function. Yeah. Yeah. The comment is could be improved. Yeah? [ Inaudible ] So there's three different -- what do you mean, sorry? [ Inaudible ] Not so much, but it depends on the implementation, really, on that case. So, yeah. So just go through them. So this one here is a set variable name. Why call it X when you could call it temp? As you said, this comment here isn't very clear. This one here is Z file. You usually only save capital letters for constants, and it also goes against your variable naming. It should all be consistent. So, I mean, this is a function anyway. But this one isn't all caps. This one is, but mainly save all capitals for constants. You say float A when actually X is a double. So you're going to lose some -- sorry, I'm not feeling great today. So you're going to lose some precision there. And then this here, you shouldn't really hard code values in. You should assign that 3.8883 to a constant variable. And then, yeah, the indentation isn't good for readability. And then this last one here does X need to be public. You don't really ever need it public. You can just do set and get functions. So you're not accidentally going to change the temperatures of X. So system testing as opposed to what we just did then, which was -- this is a software inspection. We just read through the code and we've got -- there's a couple of problems here. You don't need the program to be in an executable state to do this. You can just do this whenever. But system testing requires that it is ready to be -- that your program is -- can be executed. So that is an advantage of incremental development. So defining your tests, writing your code -- well, writing your code to do one part of your program and then test that program. Because once you've done that, then you know that part of your program is ready and it works. So then you can move on to the next one without worrying too much that the previous iteration of your program is broken. So you don't -- write out the whole program, go and test it and find there's a billion bugs. You can actually just go one by one and do like 100 bugs at a time. And you know you don't have to go back and fix that one. So, yeah, new functionality is checked as it added to the system. And then you would still have to perform regression testing, but we'll talk about that later on. And you can use real data as you're developing your -- as you're incrementally developing your program. And so you can observe any anomalies in the output as the program is developed. So here you can see how static verification and dynamic validation apply to parts of the software process. So static verification can be done throughout every single part. It doesn't require a program or a program, in fact. You can do it at any point throughout your program, whereas dynamic validation can only be applied to actual code. So if you make a prototype, you can validate that. And then when you actually have your program, you can validate that as well. I don't think that should say dynamic validation. I think it should say dynamic verification. Actually, it should say dynamic verification, so ignore that. So when you're testing programs, they reveal errors, but not -- they reveal the presence of errors, but not their absence. So, yeah, so a successful test is actually where you manage to break your program when you find an error in it, because if you do find one, that means you can fix it and then don't have to worry about that problem anymore. Simple. So when program testing is the only thing you can do as a validation technique to make sure that you've met nonfunctional requirements, you can't really do that. Functional requirements, you can look in your design and make sure it's all fine. But you can't do that with nonfunctional requirements. How do you know that your system needs five milliseconds to respond? You can't do that unless you actually have a program. So when you're testing your program as well, you should also use it with static verification as well to make sure you've got the full verification and validation coverage. So when testing with -- sorry. When testing with Agile, so when you've got these scrums, you set out how many -- like how many sprints you're going to do. You can develop those sprints with testing in mind. So you would say, okay, for this sprint, we need to be able to have the program do this, this, this and this. And then you've already got your test because you've already said what it needs to do, and then you can develop your program to make sure it meets those tests. So that actually meets the specification given. So then, yeah, then you can target your code to develop. Then you can -- when you're coding, you can just develop solely to pass that test. That means that the test is more valid because you've actually based it off the specification. Make sure that the code is always tested as well in chunks. And if you code just to do the test, you don't -- you got tunnel vision essentially. It means that you can just develop the things that you need to test. You don't need to think about anything else. And therefore, your code might be simpler. And it will actually be closer to the specification because if you write your test to meet the specification and then code to meet those tests, then your code will probably reflect the specification pretty well. So there's different types of testing. There's defect testing, which is just let's test the program and look for defects. And then statistical testing as well where your tests are done to reflect the frequency of user inputs to make sure that you know can the software actually handle this many inputs. Yeah. So the whole point of verification validation, it should allow you to be confident in your programs -- in your program's ability to actually work. So is it actually fit for purpose? But no, it doesn't mean that it is free of defects because as earlier, testing can show the presence of errors but not their absence. So that all depends on different factors and the degree of confidence. So you have software functions, so you can have a high degree of confidence in your program if it actually does what it's supposed to do. This one I don't think is that true anymore, but users sometimes have a low expectation of software and are willing to tolerate some system failures, but I don't think -- yeah, that is very true. And the marketing environment, it allows you to compare your program to other competing softwares so that you can make it -- if you're thinking about maintainability, you want your program to be maintainable. So if you're doing verification and validation, then you can actually check that your program can evolve and more verification on that side. When you're looking at your code, you can go, well, hang on, we've got 20 years of planned deployment. We should change a couple of these variables so we're ahead of the market. So defect testing and debugging are two different things. So whereas verification and validation are used to find the presence of defects in your code, debugging is concerned with the opposite side of actually going in and locating and repairing these errors. I suppose on a formal level, debugging involves forming a hypothesis about your program behavior. So you go, well, my program should do this. And then testing that behavior and finding where your error is. I suppose formulating a hypothesis could also go, okay, well, if my program is doing this and this correctly, then there should be a problem in this function instead of the other two. And I suppose if you ever do debugging, you think of that naturally without really thinking about it, but that's what you do formally. But debugging is kind of like the Wild West. Everybody does it differently. There is no step by step guide, really. Some people use debugging tools. Some people just put print and then maybe, why does this not work? Or you can be more abrasive than that. So, yeah, most of the time it depends on looking for patterns and then your skill to be able to look at what's going on in your code. Whenever you write programs in Java or any language, really, you probably come across errors. If you haven't, then you're a very good programmer. Are they syntactic or semantic errors that you're fixing with debugging, really? What do you think? Yeah? What, sorry? It isn't really. If you have a syntax error, your compiler should tell you beforehand. I will say syntax error, you can't do that, mate. But with semantic, that's more, semantic is more logical errors. Why is this array going out of bounds? That's more runtime, actually. That would still break your program. But semantic, more logical errors. So what is your code written in the wrong output? Have I put an if statement in the right place and everything? So interactive debuggers, as I just said, are very nice because they allow you to step through your program at runtime. They are very good. So there's, so back onto syntax and semantic. This is a syntax area missing a semicolon, so if that happens, your compiler should tell you you don't really need to debug syntax errors because if the compiler can't compile the code and it can't match the language, then you don't need to look for that. But semantic error, this should be a double equals because you're actually just assigning one to A in that case. So, yeah, syntax error is caught by the compiler. Semantic error, also called logical error. But the program doesn't provide the expected output. So that requires going into your program and looking, okay, why doesn't it meet the correct output? Where's the error in my code? So that's the one you would debug for semantic error. And, yeah, they are much harder to detect than a syntax error. So if you find an error, first thing you're going to want to do is just fix it and then once you've fixed it, a few days you can move on. But there might be a chance that once you fix that error, it's now broken something else in your code. That is why you need to do regression testing when that happens. So you need to do, you might want to do because it's not always feasible. So regression testing involves retesting everything after you fix that error, which is quite annoying if you've got 100 tests and then you find one bug and then you fix it and then you've had to do 200 tests. And if you're actually trying to get the product out on a timely manner and you're fixing all these bugs and retesting, it can be a bit of a pain really. So you might not want to do it, but it should be done theoretically. So, yeah, there's a large proportion of fault repairs introduce new errors or are incomplete, especially if you have spaghetti code. So this is the debugging process. You test your results and then you find an error and then by using the specification, you design an error repair, repair the error and then you do it again. You just retest everything and then you might and then this might have like an hour of going back to test results. So with verification, validation, planning, it's important that you do it carefully so you can get the most out of the testing and inspection process. So you should really plan for your verification and validation early on in the in the development process. Probably in I mean, your most basic tests will be like your success criteria as you're writing out the specification. So my program must do this, this, this, this, this, this, this, this. And then that's your first lot of tests. And then as you go into design, you might make more lower level tests. And then once you've done that, you can then when you're doing your implementation, you can then start implementing around those tests. It should have a balance between the two, between static verification and testing. So. Okay, yeah. So your test planning should define standards for the test process rather than describing the actual product tests. So this is the B model of development. So it shows how your test plans should be derived from the system specification and design. So you do your requirements specification and then your system specification. And then in that you can do your acceptance test plan and you do your system specification, your system design. You can do your system integration test plan and then your systems that have detailed design as you go into your. You go rather than the whole system integration, you start doing your subsystem integration and looking at smaller parts. And then once you do your module and unit code testing, you can do your subsystem integration tests and the rest of the tests until you eventually put out the service. So this is the B model of development. So the structure of a software test one. So when you're actually creating the test plan consists of all these following. So the actual testing process. So what are the major parts of testing? Are you going to do? The requirements traceability. So you need to make sure that all your tests cover all the operations of your requirements. So all the requirements should be individually tested. The tested items. So you going to mention everything, every single object or parts of your system that you're going to test the testing schedule. So how many people are going to do run these tests? How long should they be running these tests for? And all the way down the line. Test recording. So it's no good really saying that you've done a test. You need to record the input and the output of that test so that somebody else in your thousand employee company can see what you did. Because we can't all read minds. The hardware and software requirements. So what software tools does this test need? And also do you what what hardware do you need to run this test? So I don't know if you build in a large language model and you want to test it. You probably would want some GPUs. That's where the hardware utilization comes in. And then any constraints as well. So any constraints that affects the testing process. So. Probably a time constraint. Yeah. Time constraint would be a good one for testing. Do you have enough time to actually do those tests? So back on to software inspections. They're basically a peer to peer review. So it involves people looking at the. The source representation. So looking at. Like your design documents and your actual code and discovering anomalies and defects. This peer review is just looking at code and requirements and everything. So it doesn't actually need execution. You can apply it to any part of your. System. Yeah. Your software development process. So requirements, design, etc. And it's very effective for discovering errors. So with so this idea again comes back to not needing to execute the code so you can inspect. Incomplete versions of the code. Make sure that it's all working so far that you got correct variable names you can keep maintainability. Inspections. So yeah, that's why I just said maintainability. So as well as discovering defects, you can see actual problems in your code like so that your company has a defined program standard. And they like you to name variables in certain ways. It allows you to go through and make sure that everything is complying with those standards and that is portable and maintainable as well. And again, poor programming style and can be identified with software inspections. So. Inspection success will involve finding many, many different defects. So you may discover multiple in a single inspection. So in when you're doing testing. One defect may mask. Another. So in testing you need to even need to retest and make sure that everything is still working up after you fix an error, but that doesn't really happen in software inspection. And they you reuse domain and programming knowledge, so you're looking at the same domain and it means that review is if they've seen one error. It means that they might be able to identify if they find another error later on, it might make it easier to identify. So inspections and testing are complementary. And not posing. Verification techniques. Both should be used during the DMV process. Yeah, inspections check performance with the specification, but not conformance with the customers real needs. And they can not check non functional characteristics such as performance and usability. So you need to do testing for checking performance and reusability. So the key points from this lecture are verification shows conformance with specification and validation checks that the user's needs are met. You should use test plans when designing all these tests. Program inspections are very effective at discovering errors. I would say the only downside is not everybody wants to read code and everything all the time. Sometimes you just want to code. It's not as fun, unfortunately, but very effective. And different types of systems and software development processes require different levels of validation. Are there any questions? Cool. Thank you very much. I will see you next week.