[SIDE CONVERSATION] OK, good afternoon. Please settle down. OK, today's session, I'm going to wrap up-- there's the attendance code for today. I'm going to wrap up this discussion on security requirements. Remember, systems require security because they protect data, they protect valuable things, they protect money, and a lot of people want to steal money. And there's a whole load of different areas that I briefly introduced at the end of the last lecture-- confidentiality, integrity, authentication and authorization, non-repudiation, and availability. We're going to discuss how we're going to determine what is considered to be a good level of security, so what we actually need from this. So the first thing I'm going to do is look at confidentiality. Confidentiality broken up into two main approaches-- keeping stuff secret. OK, so one thing you can do is you can take your data and you can encrypt it. And encryption is the hardest security you can do. Once you've scrambled up something using an effective algorithm, it is very hard for somebody to find out what the contents of that is. It's never impossible because there are approaches to use brute force to break or crack a key. The key length for that has to be long enough to make sure that in the future, far enough in the future, that data is also kept secure. So if that data is just information on a credit card, in a year's time, or like three years' time depending on the expiry date of that credit card, a credit card will be invalid. Evidently, therefore, you have to keep that data secure for a good three years. Now remember, just because you can't decrypt it in real time today doesn't mean that in two years' time you cannot decrypt it. And the reason for that is that hardware is getting faster and faster and faster. That's according to Moore's law, about every 18 months, it doubles in power. And there are other advances in computation, so things like quantum computing. So if you look up any algorithm nowadays, they won't just discuss whether it is secure in terms of classical computing. It's the standard von Neumann machine of computing. They will discuss whether it is secure using quantum computing, using qubits. So for example, there is an encryption algorithm called AES, Advanced Encryption Standard. That is considered to be quantum secure. But there has been discussions of whether something like RSA, which is an asymmetric cipher, a slightly different structure, is also secure. For it to be secure under quantum computing, somebody has to devise an algorithm which works with a quantum computer to crack it. So quantum computers don't use the standard brute force approach. They have to use an approach that is designed for quantum computers. So for example, for RSA, there is an algorithm called Shor's algorithm, which is designed specifically to factorize very large numbers. And if you can factorize very large numbers quickly, you can crack RSA. Factorization of very large numbers is considered to be a mathematically very hard problem. Nobody's worked out an easy way to do it, basically. And that's what its security depends on. OK, so for encryption, it's very important you think of the key length and the algorithm used. Never work with an algorithm that is your own personal pet algorithm. Always use standardized algorithms for two reasons. One, they've been devised by people who know what they're doing, and devising encryption algorithms is extremely difficult. And two, they're nearly always publicly published. And once it becomes a global algorithm, everyone wants to get their prize by cracking it and working out a shortcut to actually breaking that algorithm, particularly things like AES. OK, so AES, lots of people have tried to attack that, and they haven't determined that yet. So always use, with algorithms, you always want to use the most regular one, the one that's standardized, the one that everybody recommends. Don't go off on some sort of thing where you say, oh, well, I think this one's better. Work with what's recommended. The other type of security you can have for confidentiality is merely permission. So you can get a file on an operating system, and you can just say, nobody's allowed to read this file. You can mark that on the operating system. You can do that yourself. That is very soft security, because it can easily be cracked by somebody just dragging out the hard disk from the back of your machine and then reading the data on the hard disk, regardless of what the permissions are. So that's not really going to hack it. In general, the data has to be kept secure in different circumstances. So when the data is stored, serialized-- it's called serialized, where you save information to a database or a hard disk-- it must be encrypted in storage. And it also must be done if that's in intermediary storage. So if I send an email to somebody else-- remember, that email just doesn't end up on my computer and their computer. It ends up on any mail server computer shoved between the two of us. So whatever service you use, even if the end-to-end point is very, very secure over the network, remember, your message can be sat there with somebody snooping your message. So it must be encrypted before it leaves your machine, and it only must be decrypted when it's at the receiver's machine, where they can read it. So it must be done before it goes. The other possibility is that you provide security on the link itself. So there are securities like WEP, which provides security over Wi-Fi links. But they don't really provide you any security at all. They're usually there because as soon as it gets to the other end of the link-- my message now if I attach my phone to a Wi-Fi hotspot-- as soon as it's encrypted and then gets the hotspot, it's unencrypted. It's back into its original text. It's not secure. So you cannot rely on that. That is zero security. It makes no difference in the real sense. It's provided mostly to stop people stealing the Wi-Fi bandwidth. It's mostly there to provide that kind of security, not to provide security of data. And finally, obviously, I just mentioned before, as long as it's reasonably possible. And that will change depending on what the thing is. So if it's a credit card, it will be one level security. If it's a state secret, you might want it to stay secure for 100 years. So you're going to have to consider a higher level of security. The next one is integrity. So here I'm sending a message to you. And it's vitally important that if you receive that message, but somebody has altered that message while it's on the way in the network, you know when you receive the message, it's altered. And if it's been altered, you know you can't trust the message and you ask me to send it again, and you ignore the message. It doesn't mean anything, because there's no point in having a message coming in, and I read it, and I know it's been altered, and I actually act on it, because obviously it could be a bad message. So you can't stop the message being modified, but you can make sure that you have knowledge that the message is changed. So there are approaches for doing integrity. You can do something called a cyclic redundancy check. And a cyclic redundancy check is you divide the message by a number, and the remainder is sent at the end of the message. The division's done modular too, but it's essentially a type of division by a number. And CRCs were developed for networks to deal with accidental changes in the data. So if you use something like ethernet, it puts CRC at the end of the message. And it works very well if you have errors in the message that are up to 32 bits long of random data. It's going to pick that up. And the chance of it allowing a message through which has a random number of errors in is something like 1 over 2 to the power of 32, OK, if I use a 32-bit CRC. So you think, well, that gives me some level of protection, but it's no good. And the reason it's no good is CRCs are designed to deal with errors that happen accidentally on the network. If I want to take a message, alter it, and then generate a new CRC, I can just run it through the algorithm, and I can generate a new CRC. So because there's no secret involved, it's not very effective. And equally, I can use something called a hashing function. Hashing functions are a little bit stronger than CRCs in that if I do a hashing function, it's a lot less likely to-- if I have any alteration in the message, it's very unlikely I'll get the same hashing value. But it has a similar problem to the CRCs that you know the hashing function, I know the hashing function, and any intermediary would also know the hashing function. OK, you can't just invent loads of new hashing functions hoping they're going to be kept secret. So the first approach that's going to work in any sensible way is to take your message, add on a secret that only you and the receiver know about, and then calculate the hashing function over the message plus the secret. Now, without the secret, I can't generate that hash value because remember, the secret is put just before the message. I use the secret plus the data and do a hashing value over it. OK. The problem with that is now we have to share a secret. And how do I get the secret to you? Well, somehow or other, I have to send it. Or maybe I ring you up and say, you know, the secret is this, blah, blah, blah, blah, blah. You've got a problem of transmitting things over a channel. The best approach is to use something called a hash value but then encrypt it using what's called an asymmetric cipher. An asymmetric cipher-- a normal cipher works like this. I encrypt with a key, and you decrypt with the same key. OK, that's a standard type of cipher. And in that, me and you have to have the same key. With an asymmetric cipher, I encrypt with one key, and you decrypt with a different but related key. The two of them are pairs. They're called key pairs. Why is that useful? Because I can then tell you all about the key. The key that you decrypt using, I can give to everyone in this room. Then I encrypt with my secret private key the hash value which has been generated. And then you can decrypt with the public key. And because you can't encrypt with the public key, you can't forge my signature. But you can check my signature because you're using that public key to decrypt. That asymmetry in asymmetric ciphers is what makes them so powerful in terms of dealing with the key distribution problem. Because now the key distribution problem is merely a matter of publishing the key to you and making sure that that key itself is protected. Well, I do that, believe it or not, by using a hash value that's encrypted. And this brings you back to something called public key infrastructure. So if you look at things like a website, and you see the website has a little secure padlock on the top left-hand side, that's because you've got a relationship with the website. And that website has securely distributed its public key to you. And every single message you get from then on, you can check that you're getting messages from the website that you're expected to. And it means they can't have been altered on the way. Somebody might have read them, but they're not altered. So integrity is a vital sort of core central requirement, integrity and confidentiality. [LAUGHTER] So the next one is authentication. So authentication is you determining, telling the system who you are. Very important. So the classic one is you use a username and a password. Authorization is what you're allowed to do. So once I get in, am I allowed to access this part of the system and that the other? And because integrity is very often an initial point of attack, because it's on the edge of the system, isn't it? I have the system there. Somebody tries to attack a login into the system. It's very important that this part of the system is kept secure. The problem with usernames and passwords, in a way, is that it's rather naive. If the system asks me to send my username and password, and I send it over the network, and somebody's sniffing the network, they've found my username and password. So evidently, that's a rather naive attempt at providing security. It's not going to provide security. So what can you do about that? Well, in theory, if you could securely connect to the server and encrypt all your messages between you and the server, then you can send the username and password encrypted. So that would be one possibility. The difficulty there is if I want to encrypt my username and password, me and the server have to share some secret information between us. And how do I establish my secret information with a server I've not talked to before? That's a problem. OK. That brings us back to this business with the integrity requirements and the hash value of the asymmetric cipher. So what the server does is it would send me a message, and that message would be an authenticated message, which is not going to-- which means I know that it's actually come from the server. And within that message is a key. And that key I use as part of my communication with the server. In fact, the technique that's commonly used to establish a key between two people who can authenticate themselves, but then we don't share a key between them, is something called the Diffie-Hellman protocol. Diffie-Hellman protocol, I make a random number, you make up a random number, and then we send the result of the modulus, a base value, up to the modulus of that-- up to the power of that random number, modulus another number to each other. And it's proven that because you're working with the modulus, it's very hard to get the random number back out of the modulus. And then you compute the shared secret between us. So I can send you the details of that if you're interested in Diffie-Hellman. Diffie-Hellman is a very sort of cool, clever technique that allows us to transfer a private key between two users in real time, but without the channel having to be encrypted. So the channel can be in plain text, but I can still share enough information for me to generate this secure key between us. But I can only do that if I know who I'm talking to. So I need to make sure that I'm talking to the right person, because they could be pretending to be the server. And then we could have a secure channel. And then I could be sending all my data to somebody I wouldn't. So Diffie-Hellman requires you to have an authenticated communication with the server. That brings us back to this authorization. So whenever you connect to something like eBay or Amazon or even here at the university and you do a secure login, it all requires these techniques to work together. OK. The other thing to note about this is that very often there is a support using hardware and biometrics nowadays. But the problem with hardware and biometrics is that it requires you to give up some of your private data or information. So it's not always the preferred technique. The other approach that I talked about using websites, that use of public keys, private keys, is called public key infrastructure. So you'll see this word come up again and again, PKI. PKI is that process where you're distributing the public keys to people in such a way that then they can communicate securely. PKI. The next thing is non-repudiation. So let's have a look at an example of a communication that would require some form of non-repudiation. So Bob is communicating with a broker to buy some shares in a company. So Bob sends a message to the broker. And let's say, what's the time now? About 20 past 3 on a Tuesday afternoon, October the 10th, and says, please buy lots of shares. The message goes to Bob's broker. Bob's broker buys the shares, and the share price does that. And then later on, what Bob says is, I didn't send that email. Prove it was me. If you can't prove it was him, then evidently, you can't prove they had a contract between them. The broker is left holding the shares, which have bombed in price, and Bob walks away. That's a problem, because that is an issue of non-repudiation. Even something where you're agreeing to buy a house, you're agreeing to buy a car, any contract requires two parties to agree. It can work on the other side. The share price could have gone up, and the broker could have denied receiving the message. So it's not only the denial of the sending the message, it's also a denial of receiving the message. Because if the shares went up, and the broker said, oh, well, I just got a hunch that they were going to go up, it was really Bob who knew that the shares were going to go up, the broker makes lots of money out of that. So how are we going to do the non-repudiation? In practice, the only way you're going to have successful non-repudiation is to have a server between Bob and the broker, which allows the messages to be stored as they transfer between Bob and the broker. The other thing that needs doing, as the message gets to the third party, the server in between, each message has to be appropriately authenticated by Bob. So they have to have Bob's digital signature on the message. And the messages need to be not only digital signature by Bob, but the message itself has to have the current time dampened state and date in it. Because look at the previous example. In the previous example, Bob, not only do you have to prove that Bob sent the message, but you have to prove the exact time that he sent it, because the share price is sensitive to time. So obviously, Bob might say-- he said buy lots of shares, but he might claim that he sent it further in time, a few hours later when the share price had gone down. And he'd say, so he wouldn't lose money on the shares. That would be another way. He could deny something about the message, like the time it was delivered. That's the time stamp. So the trusted broker takes in the messages from Bob and then looks at the contents of it. Then they're forwarded on to the actual share broker. Then the response comes back agreeing to the contract. That message has to be certified by the other party and also stored in the server. So you don't get a contract till all the messages have gone between the two parties. But plus, they are stored in an intermediary server. The other thing that it may require is some type of funding in what's called escrow. Escrow means you're coming to an agreement with a party, and you say, I'm going to do this for so much money. And you say, well, I don't trust when I do it. I don't trust you're going to give me the money. So what I want you to do is you're going to put the money in a bank account that's controlled by a third party that we both trust, a solicitor or something. And then when we agree that the whole thing's been completed and I've done what I expected to do, then the money is released from escrow. And escrow is commonly used-- some type of escrow is commonly used-- for larger transactions that have to be handled over the internet. The other technology that's being used more and more here is the technology that supports things like cryptocurrency. So cryptocurrency uses something called a blockchain. A blockchain is a whole series of messages that are passed publicly between a load of servers. That means that because these messages are passed publicly between all these servers, we can use that as a trusted broker. Because I can put my message on, see it going around the blockchain, and then somebody else can see it going around the blockchain, and everybody's information is stuck on the blockchain. Once a message is stuck on the blockchain for something like Bitcoin or something like that, that message in transactional terms, in Bitcoin terms, could be worth 10p. Doesn't have to be worth any large amount of money. But the important thing is it's on a public thing then. So we can link our communications to that timestamp on the blockchain. We can do that independently. And this is a common use case of blockchain technology now is to support things like financial transactions where people don't-- it's difficult for them to go there. Let's say you're trying to do a transaction with somebody who lives 10,000 miles away, and you want to send them a contract that they signed the contract, and then they get it certified and they send it back, use blockchain. You can do it instantaneously because you're using these digital signatures. The non-repudiation is also built on all these other services. So unless you have authentication and integrity, you can't do this unless you can prove somebody who they are at a certain time, and you need to have some recording and timestamping. But this is a very powerful technique to allow you to move financial transactions, which are large financial transactions, securely in a digital environment to make them possible on the web. So we've covered confidentiality. We've covered authentication, authorization, non-repudiation. And you would specify those in terms of a requirements document. You would have to specify those in terms of algorithm that you'd use, external services that you'd use in terms of public key infrastructure or blockchain, and/or possible service that you're going to use for brokering services for non-repudiation. Finally, we've got the definition of what are called availability requirements. Availability requirements mean how often your system is going to be down for each year your system's running. And it's always defined in terms of nines. So six nines means 99.9999% up. That means if you do the calculation the other way around, the maximum downtime per year is 31 and 1/2 seconds, which is quite good, not bad at all. Six nines would be considered pretty up there. If you might have a service that you would require better than six nines, maybe for a nuclear power station, 31 seconds down in a year could be a disaster. Let's say it goes down after two years and it's down for a whole minute. Maybe that would cause something to go really badly wrong. What if it was an airplane and you're talking about controlling an airplane? 31 seconds down could be the difference between landing safely and being a disaster. So we've got to decide. It depends on the context. And again, five nines, five minutes a year, four nines, 52 minutes, et cetera. So one nine is not really worth thinking about. That's not really a realistic proposition. Four nines is probably a minimum for any reasonable requirement. In practice, the nines technology is somewhat problematic. It's not always necessarily very useful. And the reason for that is, well, let's think of a system and let's think it has three nines' availability. But its all availability is always defined as 78 seconds per day. So for every day, this computer system, like a banking system, you couldn't access your account, but only for just over a minute a day. So you can use it all other times. You put your card in. And maybe that's at 3 AM, your reasonable time. So you're just using it and you go, well, I'm just going to have to wait. And it tells me, it says, please wait a couple of minutes. And it's back again online. So we can use it again. So we might be a bit embarrassed buying our shopping, but we just have to wait. We just have to wait a little bit. Imagine another system where it was five nines available, but the availability failure was once every 10 years. It didn't-- they have a different-- or it was just random. It didn't know. It wasn't structured. But it considered that failure after 10 years, it failed for 50 minutes. What you've got here is an issue between its availability as an average and its availability as a worst case. So for everything where you're discussing availability and requirements and security, always discuss worst case scenarios. And worst case scenarios should also include how it should degrade gracefully. So if you have a service that's a web service and it just stops, that's not graceful. That's a hard stop. If it just like says, time out, and you think, what's going on? If it says, we seem to be having a few problems at the moment, please log back in five minutes, we will send you an email message when we're back-- or a WhatsApp message when we're back online. That will be considered a graceful degrade. OK, so always these-- when you're putting in the requirements for these needs, make sure that your requirements allow for graceful failure. The security itself is very dependent on knowing what's happening to the system itself. OK, so you want to know, for example, if somebody-- if you've got a banking application and somebody is trying to log in as the director of a major bank at 2 AM and they keep getting the password wrong, that would be a massive red flag. If somebody gets their password wrong and they're an ordinary customer and they get it wrong a couple of times and it's 2 AM, well, they're probably just trying to send some money to somebody and they're very tired because they've been working all night. It would be quite reasonable to suppose that. So everything-- the point about this is that when you talk about security audits, everything has a context. Everything depends on the role of the person doing the action, what they're trying to do. Let's say they're sending a large amount of money between two accounts. Let's say it's 100,000 pounds. You might want to flag that up as a risky activity. And then also depending on timing. So we need to know what's going on. So we need to have audits and logs. So there are standard logs. And these are things that just show all the log ins and log outs and database access and requests on any system to provide security. Why do you need that? Because then you can find out-- let's say you do a load of security logs and somebody says, somebody hacked into my bank account last night. And let's say I live in the UK. I live just up the road here. I complain to my bank. And they look at it, and I can prove I was here in the UK. And they looked at the security log in, and it had an IP address that was from Germany. It said, oh, this IP address that was trying to log into your bank account, that wasn't in the UK. That was coming from Germany. That would be useful information. That would be the sort of information that would, one, prove I was probably telling the truth, and two, it would actually start then on working out who's trying to hack their system. Things like database access requests. On a higher level, on a more sort of segmented level, you would very often want to log all failed log ins. Because a failed log in could imply somebody was hacking the account. It could be somebody was just tired and they got their password wrong. But if they keep getting the password wrong, that indicates an issue. It indicates a problem with that account. It may indicate, you might send a message to the person saying, we're a bit worried about your account. Did you try and log in last night? Then they can say, no, I didn't. And then you put a lock on the account, and you go, oh, somebody's trying to get in there. That's of interest. So you can see how these little audits and logs, they allow the bank to do the human activities that are layered on top of the computer activities. So it doesn't just work, it's not just computer only, but it actually protects the thing. You can have things which are unusual. So let's say somebody, they never, never, ever pay for anything more than 1,000 pounds. And then suddenly, they've done a transaction of 10,000 pounds. And it's to a bank account not in this country, a foreign bank account, let's say in the United States. And people go, oh, that looks a bit strange. That would be something where they would possibly lock the transaction until they could confirm it. That kind of service. You notice what's happening here is there is a layer of computer activity which is enabling the human activity to protect your account. So it's not just one thing. What it's doing is it's going, that's unusual, stops the transaction, then a human can check it. So that's actually what happens in practice. So you might have an alert log, fail log-ins for top level, like a bank director. Those alerts are useful because it allows you to suspend the account. Or you can send a message to one of the security officials on the account, send them a WhatsApp message, and they can look at that. It's also useful for service as well. There is a model for security called the Bell-La-Pudler model. And the Bell-La-Pudler model is a way of managing-- this was developed by the United States Navy to handle the security of documentation. But you can also apply this to a computer system. So in the Bell-La-Pudler model, you have a clearance level to the document. So in this, I've got four levels defined. I've got level four, which is top secret, level three, which is secret, two, which is sensitive, and zero-- well, one, in this case, sorry-- which is unclassified. For Bell-La-Pudler, you can have as many levels as you like. You can have 10, 9, 8, 7, 6. You just choose what you need for your requirement. There's two essential rules. There's one called no-read-up, which means if I'm level one, I can't read a level two or level three document. So if somebody sends me an email, and it's marked as top secret, level four, and I'm a level one email reader, I can't read it. It would just blank it out. The email server wouldn't send me the contents of the email. It might send me the header. It might not send me anything at all. The next one is called no-write-down. It means if I'm a level four email sender, and I send a message to anyone else on the network, I can't send it at any level lower than level four. That means any information inside my mind can't be sent to someone else, because I just can't send it over the email. I could talk to them over coffee, yeah, maybe, yeah, if I managed to get-- but I can't do it over the email. The email has protected that. So a top secret-- so what you could use this for is sending email, but you could also use it with saving Word documents. If you had a Bell the Puddler model with saving Word documents, every time you saved the document, it would stamp your clearance level on the document. Every time you read the document, it would try and determine whether it was readable. Obviously, it would also involve some degree of encryption to control this, technically speaking, but in a practical sense of what you can read up and read down. There is another subject called the trusted subject, which can write documents down. Writing documents down in a general sense, in a government sense, is called declassifying information. And governments do that every year. They say, we've got these things that we thought were secret, but they're not secret anymore. Because let's say there were 30 years ago, we don't need to keep them secret. And that's called declassification. And all governments declassify all the time. It's a very common practice. It's very important. When you specify the security-- because remember, all this we've been discussing is part of requirements analysis, isn't it? So we have to work out what we're going to specify. So you want to keep it open. So you want to say, we're using encryption algorithm defined in this appendix, and then say, with possible upgrades to these possible other encryption algorithms. But you also need to have something called a security policy for the organization you're working in. So it's no good having a secure computer system if everybody is just writing their password down, and then scribbling the password up and throwing it in the bin. And then people can look through the rubbish bins outside the organization and read all the password. That's no good. So you need documents to be shredded. You need a secure disposal. You need-- how often passwords must be reset. You need people to be trained in security. So you're not allowed to use this system unless you've been trained. And then some degree of audit to see how it is. There are external standards to do with security. So this one, the payment industry data security standard, that tells you what you must do if you store-- you know, like your payment cards, like your credit card or your debit card. If that information is stored on a server in the UK, you have to comply with this standard. And if you don't comply with this standard, they will come round to your place of work, and they will shut your server down. And they want to see your source code. They want to see how the software keeps things encrypted. They want to look at your database records to check that you've encrypted all the credit card numbers. They need to see that level of detail, the people who keep that conformed. Once you've done all that, once you've done all your security requirements, you've done everything else, you need to do some checking of the requirements. So you need to check that they are valid. You need to check they're consistent. And you need to check they are complete. And you need to check they're realistic. OK, now all of these are going to apply to all the requirements we talked about so far, but they're particularly true when it comes to security requirements. So it's really important that you think about that in terms of security. The final thing is a requirement has to be verifiable. It's no good saying the system's going to run fast. You have to say the system, when we log in, always responds within 0.1 of a second. So you have to say exactly how the requirement can be checked. And ideally, the requirement can be checked automatically by some type of automatic test. And again, it's important that when you work in a contracting situation. The other thing to note about requirements is they're very often dependent on what are called scenarios. And we've looked at scenarios slightly before when we've talked about use cases. But scenarios are also very useful when we think about testing. So a scenario is a test case given in a particular situation. So one scenario would be for the ATM machine, I try and withdraw cash with a stolen credit card. Why is that useful? Because it gives me that scenario. Because now it's defined a test. And when I get to the testing phase, I know what test to do. Try and withdraw cash, but the machine has a low cash stock. Withdraw cash with a card number, blah, blah, blah. So now the developer knows what's going to happen in those given circumstances. You'll notice the scenarios are very related to those use cases. So each scenario belongs to a use case. It lives in a use case. So one path through the use case will be one scenario for the use case. And you might define 10 scenarios to test one particular use case. There is a tool, an agile tool, called Cucumber. And that allows you to link your test directly to automatically link the tests, the definition of the requirements, to the tests. So Cucumber defines your features in this language called Gherkin. And you can extract from this a series of tests. So it always starts off with a preamble here. And then it defines a series of scenarios. One scenario gives you one test. So you define 20 scenarios. You've got 20 tests up. And because this is written not in a programming language, it's written in a language that could be done by somebody without programming expertise, because it just lists what the system should do. It doesn't have any code in it, as such. These scenarios can be added to later on. And that adds more and more tests to the system. So this is one of the tools that's available to help you test your requirements and make sure your test is there. So when we take that, we press the button. We generate some test code. And the test code, some of the data is extracted from that Gherkin script to allow it to run those tests. OK, so that can be fetched. That information is fetched from here, from this one. You see where it says username. A password of pass 1, 2, 3, 4. You can see this, where it goes password. So it's going to extract the password from that. OK. So Q-Qom is quite nice, because it allows you to write the scenarios and write the specifications in some detail. And then the analysts and test team can develop that. So I'm going to finish at this point. We've got a slide to go. We'll do those in the next one, because I don't want you to run late.