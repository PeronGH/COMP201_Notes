[SIDE CONVERSATIONS] Good afternoon, everyone. There is the attendance code. I'll just give you a minute or so, 30 seconds to do that. And then we'll begin. [SIDE CONVERSATIONS] OK, so welcome back to software engineering. This week, this is the final lecture on testing, specifically software testing. So just at the end of the last lecture, we talked about cyclomatic complexity. And it just tells you the number of tests that you need to do to make sure that you test every single path in your program. This is equal to the number of tests, the test all control statements, so all your RIV statements and everything. If you draw a program flow graph, the cyclomatic complexity is equal to the number of edges, take the number of nodes, plus 2. So conditions are any type of branch in operation, such as each if statement or any type of loop as well. So those are what we consider conditions in this case. So cyclomatic complexity tells you how many tests you need to do to make sure you do all those conditions in your code. And by drawing-- oh, yeah. So that's an important thing at the bottom. It does not imply that your testing is adequate enough, because it doesn't include all the combinations of paths. It just includes going down every single path at least once. So you can make sure that all your conditions work. So it looks at this binary search. And so this is the program flow graph for that binary search. So the cyclomatic complexity, after drawing it out, we can work out just by counting how many edges there are, how many nodes there are, and then taking the number of nodes away from the number of edges plus 2. So we have 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11 edges, and then 9 nodes. So that's just 11 take 9 plus 2. And then you can-- 2 plus 2 is 4. And then you can see we have four independent paths to go down. So we can go down 1, 2, 8, 9 to get to the end. And sure enough, just being slow, 1, 2, 8, 9 is a path in our program. You can go 1, 2, 3, 8, 9. Sure enough, 1, 2, 3, 8, 9 is there. And you can do 1, 2, 3, 4, 5, 7, 2, 8, 9. And then you can do that. And then you're the same. You can branch off down there instead. So that's how you make sure you try every single path in your program at least once. But it doesn't work for all the combinations of paths. So you can't do 1, 2, 3, 4, 5, 7, 2, 3, 4, 6, 7, 2, 3, 4, 5, 7, 2, 3, 8, 9. That's not what the cyclomatic complexity is. It doesn't check all the combinations of paths that you can make in your program, just the number you need to test at least once to make sure that you have gone down every single path in your program. So when you would test this and make sure you go down every independent path, your test cases should be derived so that you do expand all the paths. So how do I ensure that I have an input that goes to 1, 2, 3, 8, 9? And what input would I need to make sure I go down 1, 2, 3, 4, 6, 7, 2, 8, 9? That's what you'd have to do. And then you can use a dynamic program analyzer along with your testings to support you to know that you have gone down every and checked every path in your program. So coming back onto integration testing, this tests your system, like your whole system or subsystems within your system. And you usually do this with black box testing. And you do this to check whether your program meets the specifications. So your test should be derived from the specification. The issue with just testing it as a whole product or like a large subsystem within your project is it's hard to find those errors. Because if you're just testing the whole thing, you don't know what you're looking for specifically. So we can use incremental integration testing to reduce this problem. So what is incremental integration testing? We have two components above test sequence one. We just have A and B as the components. And then we have three tests for that. We have test one, test two, and test three. So once you've tested that and you make sure it works, you then include component C in test sequence two. And then you do a test one, test two, test three, test four. So you do those tests again. And then once you've done that and you've tested that, that test sequence works. You then move on to test sequence three. You include more components. And you do more tests as the number of components increases. So the thing to note from that slide was it uses regression testing. Because even though we've tested A and B and we've done test one, two, and three, we still do tests one, two, and three again before we do test four. The reason for that is, as we mentioned with regression testing last time, you need to test everything in your program as you add new things to make sure that you haven't broken anything when you've added new functionality. So yes, what's up there? And you can automatically do this with what's called a test harness, which is just a testing software that allows you to generate your own tests. So JUnit is a good example for one in Java. So maybe you did that with Patrick with 1.2.2 last year, where he used JUnit when he was assessing your code grade. When you were blowdown code grade, he was doing JUnit tests, I believe. But that's how you can test by using JUnit. So you can use a test harness and allow for automated testing. So that's quite useful. But we're not looking at it anymore. That's as far as I'll mention it. So there's a couple of approaches to integration testing. That would be top-down testing, where you start with the highest level component in your system. And rather than have the lower levels, you replace them with things called stubs, which simulate the behavior of the lower components. So they're kind of like really reduced lower level components. And then you have bottom-up testing, where you do it the opposite way. You test the lowest level components first. And then you use what we call drivers to test to simulate the higher level components. And you keep on doing that until you've tested everything in your program. In practice, though, you usually use both of these. So this is a diagram to show top-down testing, where when we begin at level one, we test the highest level. And then we have these level two stubs, which, like I said just then, just simulate the behavior of the lower level components. So they're not actually the lower level components, but they have the functionality there to at least behave a bit like them. Well, mostly you want to capture it as closely as possible. But with stubs, you might find that the true simulation isn't captured. And then once you've tested the first level and you're happy with it, then you move on to testing the top level again, but this time with the actual components from level two, as opposed to the stubs. And you keep on doing that until you've tested everything in-- until you've tested all the levels of components in your program so that you know your program works. And on the opposite side, we have bottom-up testing. So you start off at level n and being the number of the lowest components, so let's say five. So at level n, you have these-- or level five, you have these test drivers, which would simulate level four. They would just behave in the same manner as the fourth level component without being the fourth level component. And then as soon as you've tested and you're happy with the lowest level, you can then move on to the lowest level and then the level above. So then you can move on to level four, and then you can simulate level three and above. And these are the two approaches to integration testing. So you can start from the top down and work your way down your program, or you can start from the bottom up and work your way up. So why use either of them? Why not just use one? In object-oriented systems, you have already decomposed a lot of your program into objects. So it's useful. It makes decomposing the tests easy, and therefore you might want to start on the lowest levels to make sure that you're capturing the correct behavior of all your objects. They're useful in real-time systems because it allows you to see what components, if you're looking at them on the lowest level first, which ones are behaving slowly. And then systems with strict performance requirements as well because you can assess how well they perform early on in testing. So that's why you would use bottom up. Now, top down is better at giving an idea in relation to how well you've designed the program. How well you've designed the program, and does it meet the specifications and everything? So top down gives you a better idea of discovering errors in the system architecture, so as a whole rather than individual components. Top down allows you to see some form of a system demonstration earlier on in the development. But if you were doing test implementation, like actually designing your test, it'd be easier to use bottom up because bottom up can be applied to the finer, lower level components of your system. Therefore, it's easier when you are implementing tests. It might be easier to use bottom up. Now, the problem with both the approaches is it can be quite difficult to actually observe the output of the tests. So you might need extra code to do so. These would just be logs and everything like that, just so you can see what each-- so you can have each object or component of your system just print into a log file, and just talking about where it's at, what it's doing right now. And that can give you more information about your testing. So now moving on to interface testing. So this takes place when you start using subsystems and singular systems start making up logic systems. The objective of interface testing is to just find errors within your interface. And it's particularly important in object-oriented programming because objects are defined by their interfaces. So you will have some interface that will have its own objects declared inside it. So you need to make sure that that interface that you've created isn't producing any errors, and all the objects inside are working fine. So here you've got your test cases, and you put them on the boundary of the interface. And the interface that contains all the objects and components within it, you'd want to make sure that they're all working fine. But you need to test that the interface is working. So there's a couple of different types of interfaces, one of them being a parameter interface, which just takes some parameters and passes it on to the correct function, or the correct procedure, rather. Same thing. Shared memory interfaces, so just allows memory to be shared throughout all the objects inside that interface. And procedures, procedural interfaces. Sorry. Yeah, sorry. Don't know what happened then. Subsystem-- but yeah, procedural interface just subsystem then encapsulates all the procedures to be called by other subsystems. So you can have just two subsystems all in between each other. And one of the interfaces contains all that information about the procedures within it. And then you have message passing interfaces as well. So subsystems request services such as procedures or attributes from other subsystems. So those are your different types of interfaces. So what are the interface errors we're looking for? One of them is interface misuse. So for example, if a calling component calls another component and it has its parameters in the wrong order, that calling component has misused the interface, has used it incorrectly. So we're looking for stuff like that. Interface misunderstanding. So if a component thinks or thinks is coded to behave and assume that an interface works in a certain way, but in fact it doesn't, then that would be an interface misunderstanding. The component isn't written to process how the interface works correctly. And then you could have a timing error. So if the called and the calling component run at a different speed, for example, there is a chance that the calling component might get out of date data before the actual called interface has actually been able to process it. So process the request. So these are the errors that you'd be looking for. So when doing interface testing, there are a couple of guidelines. The first one is to design tests so that the parameters are at the extreme ends of the range that they can accept. So if an interface only accepts five digit numbers, then you'd want to test the boundaries of those, of five digit numbers, just like we were looking at with equivalence partitions last lecture. Sorry. You always test pointer parameters with null pointers. So if you had a pointer parameter which pointed to a memory location, you'd want to give it a null pointer just to see what would happen if you did give it an empty memory address location you'd expect it to fail. You'd want to design tests so that components fail. When it comes to message passing systems, you want to do stress tests to make sure that the messages and everything will be overloaded. And in shared memory systems, you can vary the order in which components are activated. So let's say one component needs to be activated before. Another will that affect how the shared memory is accessed. Just getting a bit into low level hardware there. So yes, stress testing, you want to make sure that the tests you do, especially message passing system interfaces, you want to try and exceed the maximum loads that they can handle. The reason for this is because you want to make sure that there are no catastrophic failures. So if you perform a stress test, you'd want it to just be able to limit how much is actually being sent between these two components. You don't want a huge failure as a result. You'd want your system to be robust, so that's why you should stress test it. Yeah, that's particularly relevant to distributed systems. If you've got a network and you overload the network, you don't want the network to fail. So you need to perform stress tests to make sure that your program can actually handle-- or your group programs can actually handle not catastrophically failing if you give it too much data. Or you pass data between two-- too much data between two components. Yes, so object-oriented testing. So the components in object-oriented testing that are tested are the classes which are instantiated as objects. So you want to test the classes in your program. That's all you need to test. So those are the only components that you need to test in object-oriented program. There are larger grain when considering object-oriented, because it's more than just a single procedure. It's a group of procedures and a group of attributes. So because of that larger grain compared to just procedures, what you need to do is expand white box testing to make sure that everything is accounted for. [COUGHS] Yeah, in object-oriented systems, there is no obvious top of the system. There could be, but not all the time. So doing top-down integration testing can be difficult sometimes. So the different testing levels for object-oriented are these here. So you want to test all the operations that are associated with objects to make sure that they're functioning properly. You want to test the whole class. So once you've tested the functions inside of an object, you'd want to make sure that the whole class actually did work. Then you would test two objects together that cooperate with each other. And then after you've done that and built up the cooperations even more, you'd finally want to test the whole system itself. So first, you need to do a complete test coverage for a class. And that involves testing all the operations inside it. Just like I said, you'd first test all the operations inside an object. Then you would test setting and interrogating or getting its attributes. And then you'd want to make sure that the object behaved OK in all the possible states that it can be in. Inheritance makes it difficult to design object tests sometimes because your tests aren't just localized. If you inherit a class and you test one object, that test might behave. It's not localized anymore. So the object test that you might design for one class might not work for another one because even though it's inherited, it's made a couple of different changes. And it just makes it a bit more confusing. So we're going to look at this weather station object interface now. So we need to design test cases for all the operations. And one way you can do that-- objects have states. So if you had a Boolean inside an object, it could have two states. It could either be 0 or it could either be 1. So there are two states to that object. Now as you develop more states, you're getting to-- at the lower level, you could have millions of states. But at the higher level, it would have a couple of different states. So the nice thing about objects having states is that you can draw state diagrams to model their behavior and how their state changes. So for example, in this weather station, you could write these as testing sequences. But they more or less demonstrate states of the object. So you could have the weather station shut down. Then you could have it waiting. And then you can have it shut down. You probably expect that to fail because you can't shut it down and wait and shut it down again. Or at least you shouldn't be able to. Another testing sequence you can do is you can have it waiting. Then you can calibrate and test some data. Then you can transmit the data and then wait. And then you could wait, collect, wait, summarize, transmit the data, and wait again. So those are the different object states. So drawing state transitions for testing can be useful. When describing an object state and seeing how it should be able to change. So levels of integration in an object-oriented system are difficult to figure out. And that is because you might have some objects over here and some objects over here. They're all working together. It might not be clear which one is at the top, which one is at the bottom. So a useful thing we can do is cluster testing. And that's concerned with integrating and testing clusters of cooperating objects. So objects that talk between each other. Now, the only way you can know which objects talk to each other is by knowing the actual system itself. So you use the knowledge, the operation of the objects. So that you can check system features. And specifically, the system features that those clusters of cooperating objects implement. So the approaches to cluster testing. The first one is use case or scenario testing. And this is just based on use cases and how the program should work. So your tests are completely based on how a user should interact with it. That's always a good thing because that means that you've been able to test the specification that you received actually works. You could do thread testing. So you test the system's response to events as processing threads through the system. So not to be confused with actual program thread switcher software concepts where you have two different programs on one CPU or two different versions of a program on a CPU. What you're doing is you're testing the system as processing a thread like an actual order of instructions. And object interaction testing. So you test sequences of object interactions. So we can identify scenario based testing test cases by looking at use cases and also interaction diagrams because interaction diagrams allow us to model that dynamic behavior. So that shows how the objects should work in that given scenario. So let's consider this scenario where the station system where the report is generated. So this person over here who I assume works with the system requests the COMS controller for a report. The COMS controller requests the weather station for a report. And the weather station summarizes the weather data. And then all that's returned all the way back. So by doing this, we've done a scenario based test because we've seen the scenario. We know how it works because we have the sequence diagram. And we want R. Then we want to check that. And we want R. Then we want to check that our system behaves the same way. So the thread of methods executed is the COM controller makes a request. The weather station calls a function report. And the weather data summarizes. So even though this person over here, but the COMS controller-- oh, sorry. The person calls the request function in COMS controller. COMS controller calls the weather station report for a procedure. And the weather station calls the weather data. Can you summarize that, please? So your inputs and outputs. So your input for this test would be the report request. And with all the knowledge that you need to provide the final output by the weather station and the final output of a report. You can do this by creating raw data and ensuring that it's summarized properly. You'd probably want to start off with, especially in a weather system, a small amount of data to make sure that that small data works. And then eventually increase it and check that it works properly. And you can use the same raw data so that you can test the weather data or object to make sure that that is working properly. So the key points to this lecture, you need to make sure you test from the very start with psychometric complexity. Your test coverage ensures that all statements have been executed at least once. That interface defects arise because of specification, misreading, misunderstanding, or timing assumptions. And then that you need to test all object classes, all their operations and attributes and states that they can be in. And you can integrate object-oriented systems. So do integration testing. Sorry, bring object-oriented systems together into clusters and then test them together, those objects that talk to each other. Thank you very much. And if anybody has any questions, please feel free to come down and put the-- [DING] --thing back on the board. Thank you very much.